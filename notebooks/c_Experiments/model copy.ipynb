{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5811e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e08fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder, RobustScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from feature_engine.timeseries.forecasting import LagFeatures, WindowFeatures, ExpandingWindowFeatures\n",
    "from feature_engine.datetime import DatetimeFeatures\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54865cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Annotated, Tuple, Dict, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d969eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dde7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774511ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fe7a110",
   "metadata": {},
   "source": [
    "<center><b>Process Data</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2265884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(data_source: Annotated[str, 'data_source']) -> Annotated[pd.DataFrame, 'data']:\n",
    "    \"\"\"\n",
    "    Ingests data from a given path.\n",
    "\n",
    "    Args:\n",
    "        data_source: The path to the data.\n",
    "\n",
    "    Returns:\n",
    "        The data as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Reading data from {data_source}\")\n",
    "        data = pd.read_parquet(data_source)\n",
    "        logging.info(f\"Data read from {data_source}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading data from {data_source}: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce909ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data: Annotated[pd.DataFrame, 'data']) -> Annotated[pd.DataFrame, 'cleaned_data']:\n",
    "    \"\"\"\n",
    "    Clean the data by removing duplicates, null values, and converting columns to appropriate types.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned data. None if an error occurs.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Cleaning data...\")\n",
    "        data.drop_duplicates(keep='last', inplace=True)\n",
    "        data.dropna(inplace=True)\n",
    "        data.drop(columns=['client_id', 'CID', 'Base Size'], inplace=True)\n",
    "        \n",
    "        # format the date time\n",
    "        data['date'] = pd.to_datetime(data.date).values\n",
    "\n",
    "        # Sort\n",
    "        data.sort_values(by='date', inplace=True)\n",
    "\n",
    "        # renaming cols\n",
    "        data.columns = [col.lower().strip().replace(' ', '_')\n",
    "                        for col in data.columns]\n",
    "        data.rename(\n",
    "            {'area_(km)^2': 'area_km2', 'population_(approx.)': 'population',\n",
    "             'literacy_rate_(%)': 'literacy_rate_perc'},\n",
    "            axis=1, inplace=True)\n",
    "\n",
    "        # optimizing for memory\n",
    "        for col in data.select_dtypes('float64').columns:\n",
    "            data[col] = data[col].astype('float32')\n",
    "\n",
    "        for col in data.select_dtypes('int64').columns:\n",
    "            data[col] = data[col].astype('int32')\n",
    "\n",
    "        # lType conversion\n",
    "        data['literacy_rate_perc'] = data.literacy_rate_perc.astype('float32')\n",
    "        data['kpi'] = data.kpi.astype('float16')\n",
    "        data['tmtm'] = data.tmtm.astype('float32')\n",
    "\n",
    "        # rename date -> timestamp\n",
    "        data.rename({'date': 'timestamp'}, axis=1, inplace=True)\n",
    "\n",
    "        logging.info(\"Data cleaned.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error cleaning data: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "978bb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_aggregate_data(\n",
    "    data: Annotated[pd.DataFrame, 'cleaned_data']\n",
    ") -> Tuple[Annotated[pd.DataFrame, 'target'], Annotated[pd.DataFrame, 'static features'], Annotated[pd.DataFrame, 'aggregated application_group table'], Annotated[pd.DataFrame, 'aggregated uses tabe'], Annotated[pd.DataFrame, 'aggregated mkt table']]:\n",
    "    \"\"\"\n",
    "    Encode categorical features of the data.\n",
    "\n",
    "    Args:\n",
    "        data: Dataframe containing the categorical features.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe containing the encoded categorical features OR None.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info('Encoding categorical features...')\n",
    "        \n",
    "        # HASH FEATURES category\n",
    "        data['category'] = data.category.apply(\n",
    "            lambda cat: {'Domestic': 1, 'Power': 0}[cat])\n",
    "        data['grade'] = data.grade.apply(\n",
    "            lambda cat: {'Grade1': 1, 'Grade2': 2,\n",
    "                         'Grade3': 3, 'Grade4': 4}[cat]\n",
    "        )\n",
    "        data['ecoind'] = data.ecoind.apply(\n",
    "            lambda cat: {'Medium': 2, 'High': 4, 'Low': 2, 'Poor': 1}[cat]\n",
    "        )\n",
    "\n",
    "        # OneHot Encoding\n",
    "        data = pd.get_dummies(data, columns=['division', 'region'])\n",
    "        \n",
    "        # renaming cols\n",
    "        data.columns = [col.lower().strip().replace(' ', '_')\n",
    "                        for col in data.columns]\n",
    "        \n",
    "        for column in data.select_dtypes('bool').columns:\n",
    "            data[column] = data[column].astype(int)\n",
    "    \n",
    "\n",
    "        # optimize for memory\n",
    "        for col in data.select_dtypes('int64').columns:\n",
    "            data[col] = data[col].astype('int32')\n",
    "\n",
    "        # Aggregate targets by outlet_id\n",
    "        targets = data.pivot_table(index=['timestamp','outlet_id'], aggfunc={\n",
    "                'net_price': 'mean',\n",
    "                'qtym': 'mean',\n",
    "            }\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Aggreate static feature by outlet_id\n",
    "        static_features = data[['timestamp','outlet_id','wire', 'rm',\n",
    "           'fy', 'grade', 'noc',\n",
    "           'dfc', 'area_km2', 'population', 'literacy_rate_perc', 'pcx', 'excnts',\n",
    "           'exach', 'trc', 'tlcolt', 'tmtm', 'ecoind', 'sf', 'sop', 'pminx',\n",
    "           'tms_cr', 'mas', 'kpi', ]].groupby(by=['timestamp','outlet_id'],).mean().reset_index()\n",
    "        \n",
    "        # aggreated appliatin group by outlet_id\n",
    "        application_group = pd.DataFrame(columns=['General', 'Moderate', 'Rich', 'Industry'])\n",
    "        for outlet in data.outlet_id.value_counts().index:\n",
    "            ratio = data.loc[data.outlet_id==outlet, 'application_group'].value_counts(normalize=True).to_dict()\n",
    "            application_group.loc[outlet] = ratio\n",
    "        application_group = application_group.fillna(0).reset_index().rename(columns={'index':'outlet_id'}).astype(np.float32)\n",
    "        \n",
    "        # Aggregated uses by outlet_id\n",
    "        uses = pd.DataFrame(columns=[\n",
    "            'House Wiring', 'Fan & Lighting Connection',\n",
    "            'Air Condition & Washing Machine, Heavy Item', 'Lift & Heavy Item',\n",
    "            'Earthing', 'Industry, Machineries'\n",
    "            ]\n",
    "        )\n",
    "        for outlet in data.outlet_id.value_counts().index:\n",
    "            ratio = data.loc[data.outlet_id==outlet, 'uses'].value_counts(normalize=True).to_dict()\n",
    "            uses.loc[outlet] = ratio\n",
    "        uses = uses.fillna(0).reset_index().rename(columns={'index':'outlet_id'}).astype(np.float32)\n",
    "        \n",
    "        # Aggregated mkt ratio by outlet_id\n",
    "        mkt = pd.DataFrame(columns=\n",
    "            ['Urban', 'Rural', 'Semi Urban', 'Others']\n",
    "        )\n",
    "        for outlet in data.outlet_id.value_counts().index:\n",
    "            ratio = data.loc[data.outlet_id==outlet, 'mkt'].value_counts(normalize=True).to_dict()\n",
    "            mkt.loc[outlet] = ratio\n",
    "        mkt = mkt.fillna(0).reset_index().rename(columns={'index':'outlet_id'}).astype(np.float32)\n",
    "        logging.info('Encoding categorical features completed.')\n",
    "        return targets, static_features, application_group, uses, mkt\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error encoding categorical features: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d32640a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddTemporalFeatures(targets: Annotated[pd.DataFrame, 'encoded data']) -> Annotated[pd.DataFrame, 'temporal features']:\n",
    "    features_to_extract = [\n",
    "        \"month\", \"quarter\", \"semester\", \"week\", \"day_of_week\", \"day_of_month\",\n",
    "        \"day_of_year\", \"weekend\", \"month_start\", \"month_end\", \"quarter_start\",\n",
    "        \"quarter_end\", \"year_start\", \"year_end\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        logging.info(f'==> Processing AddTemporalFeatures()')\n",
    "        temporal = DatetimeFeatures(\n",
    "            features_to_extract=features_to_extract).fit_transform(targets[['timestamp']])\n",
    "        # for col in temporal.columns:\n",
    "        #     data.loc[:, col] = temporal[col].values\n",
    "        logging.info(f'==> Successfully processed AddTemporalFeatures()')\n",
    "        return temporal\n",
    "    except Exception as e:\n",
    "        logging.error(f'==> Error in AddTemporalFeatures()')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698c7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddLagFeatures(targets: Annotated[pd.DataFrame, 'after added temporal features']) -> Annotated[pd.DataFrame, 'Lag features']:\n",
    "    \"\"\"\n",
    "    Add lag features to the data.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Adding lag features to the data.\")\n",
    "    try:\n",
    "        # Add Lag  Feature\n",
    "        lagfeatures = LagFeatures(variables=None, periods=[3, 8, 16, 24], freq=None, sort_index=True,\n",
    "                                  missing_values='raise', drop_original=False)\n",
    "        lagfeatures.fit(targets[['timestamp', 'net_price', 'qtym']])\n",
    "        features = lagfeatures.transform(\n",
    "            targets[['timestamp', 'net_price', 'qtym']])\n",
    "        # for col in list(features.columns)[3:]:\n",
    "        #     data[col] = features[col].values\n",
    "        logging.info(f'==> Successfully processed add_lag_features()')\n",
    "        return features.drop(['timestamp', 'net_price', 'qtym'], axis=1)\n",
    "    except Exception as e:\n",
    "        logging.error(f'in The add_lag_features(): {e}')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e2246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddWindowFeatures(targets: Annotated[pd.DataFrame, 'After lag features added']) -> Annotated[pd.DataFrame, 'window features']:\n",
    "    \"\"\"Add window features to the dataframe\n",
    "\n",
    "    Args:\n",
    "        data (Union[dd.DataFrame, pd.DataFrame]): The dataframe to add window features to.\n",
    "\n",
    "    Returns:\n",
    "        Union[dd.DataFrame, pd.DataFrame]: The dataframe with window features added.\n",
    "    \"\"\"\n",
    "    logging.info(\"Adding window features to the dataframe\")\n",
    "\n",
    "    try:\n",
    "        windowfeatures = WindowFeatures(variables=None, window=24, freq=None, sort_index=True,\n",
    "                                        missing_values='raise', drop_original=False)\n",
    "        windowfeatures.fit(\n",
    "            targets[['timestamp', 'net_price', 'qtym']])\n",
    "        features = windowfeatures.transform(\n",
    "            targets[['timestamp', 'net_price', 'qtym']])\n",
    "        # for col in list(features.columns)[3:]:\n",
    "        #     data[col] = features[col].values\n",
    "        logging.info(f'==> Successfully processed add_window_features()')\n",
    "        return features.drop(['timestamp', 'net_price', 'qtym'], axis=1)\n",
    "    except Exception as e:\n",
    "        logging.error(f'in add_window_features(): {e}')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c0f77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddExpWindowFeatures(targets: Annotated[pd.DataFrame, 'after added temporal features']) -> Annotated[pd.DataFrame, 'added Expanding Window features']:\n",
    "    \"\"\"Add Expanding Window Features to the data.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data.\n",
    "    Returns:\n",
    "        pd.DataFrame: The data with added expanding window features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        expwindow = ExpandingWindowFeatures(\n",
    "            variables=None, min_periods=7, functions='std', \n",
    "            periods=7, freq=None, sort_index=True, \n",
    "            missing_values='raise', drop_original=False\n",
    "        )\n",
    "        features = expwindow.fit_transform(targets[['timestamp', 'net_price', 'qtym']])\n",
    "        \n",
    "        # # \n",
    "        # for col in list(features.columns)[3:]:\n",
    "        #     data[col] = features[col].values\n",
    "        return features.drop(['timestamp', 'net_price', 'qtym'], axis=1)\n",
    "    except Exception as e:\n",
    "        logging.error(f'in The add_expw_features(): {e}')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "312f9bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_features(\n",
    "    targets: Annotated[pd.DataFrame, 'targets'], \n",
    "    static_features: Annotated[pd.DataFrame, 'static_features'],\n",
    "    application_group: Annotated[pd.DataFrame, 'application_group'],\n",
    "    uses: Annotated[pd.DataFrame, 'uses'],\n",
    "    mkt: Annotated[pd.DataFrame, 'mkt'],\n",
    "    \n",
    ") -> Tuple[Annotated[pd.DataFrame, 'features'], Annotated[pd.Series, 'target'], Annotated[BaseEstimator, 'imputer']]:\n",
    "    \"\"\"Merges All Features into One.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data.\n",
    "    Returns:\n",
    "        pd.DataFrame: The data with added expanding window features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f'==> merging features...')\n",
    "        \n",
    "        # Generate outlet wise timeseries_features\n",
    "        timeseries_features_outlet_wise = pd.DataFrame()\n",
    "        for outlet_id in targets.outlet_id.value_counts().index:\n",
    "            outlet_wise = targets.loc[targets.outlet_id==outlet_id]\n",
    "            temporal = AddTemporalFeatures(outlet_wise)\n",
    "            lag_features = AddLagFeatures(outlet_wise)\n",
    "            window_features = AddWindowFeatures(outlet_wise)\n",
    "            exp_window_features = AddExpWindowFeatures(outlet_wise)\n",
    "            outlet_wise_features = pd.concat([outlet_wise[['timestamp','outlet_id',]], temporal, lag_features, window_features, exp_window_features], axis=1)\n",
    "            timeseries_features_outlet_wise = pd.concat([timeseries_features_outlet_wise, outlet_wise_features], ignore_index=True)\n",
    "        \n",
    "        # Merge outlet wise timeseries_features\n",
    "        targets.merge(timeseries_features_outlet_wise, on=['timestamp','outlet_id',], how='inner')\n",
    "\n",
    "        \n",
    "        # Merge application group, uses, mkt\n",
    "        data = targets.merge(\n",
    "            application_group, on='outlet_id', how='inner'\n",
    "        ).merge(\n",
    "            uses, on='outlet_id', how='inner'\n",
    "        ).merge(\n",
    "            mkt, on='outlet_id', how='inner'\n",
    "        ).merge(\n",
    "            static_features, on=['timestamp', 'outlet_id'], how='inner'\n",
    "        ).merge(\n",
    "            timeseries_features_outlet_wise, on=['timestamp', 'outlet_id'], how='inner')\n",
    "        \n",
    "        # Impute Missing Values\n",
    "        target = data['qtym']\n",
    "        features = data.drop(columns=['timestamp', 'net_price', 'qtym'])\n",
    "        imputer = KNNImputer(n_neighbors=5).fit(features)\n",
    "        features = pd.DataFrame(imputer.transform(features), columns=features.columns)\n",
    "        features['timestamp'] = data.timestamp\n",
    "        del data\n",
    "        return features, target, imputer\n",
    "    except Exception as e:\n",
    "        logging.error(f'==> Error when merging features: {e}')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e93e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(features: Annotated[pd.DataFrame, 'features to scale'],) -> Annotated[pd.DataFrame, 'standardized features']:\n",
    "    \"\"\"Scaling step.\n",
    "    Args:\n",
    "        data: Input data.\n",
    "    Returns:\n",
    "        Normalized data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f'==> Processing scale_data()')\n",
    "        # Assuming the data is a pandas DataFrame\n",
    "        scaler = RobustScaler(\n",
    "            with_centering=True,\n",
    "            with_scaling=True,\n",
    "            quantile_range=(25.0, 75.0),\n",
    "            copy=True,\n",
    "            unit_variance=False,\n",
    "        )\n",
    "        scaler.fit(features)\n",
    "        features = pd.DataFrame(scaler.transform(features), columns=features.columns)\n",
    "        scaler.fit(features)\n",
    "        \n",
    "#         # save Scaler model\n",
    "#         joblib.dump(scaler, os.path.join(config.ARTIFACTS_DIR, 'scaler.pkl'))\n",
    "#         logger.info(\n",
    "#             f'Scaler model saved to {os.path.join(config.ARTIFACTS_DIR, \"scaler.pkl\")}')\n",
    "#         features.to_parquet(config.FEATURE_STORE, index=False)\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        logging.error(f\"in scale_data(): {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c4d9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    features: Annotated[pd.DataFrame, 'features'],\n",
    "    target: Annotated[pd.Series, 'target'],\n",
    "    test_size: float = 0.25,\n",
    "    random_state: int = 33\n",
    ") -> Tuple[Annotated[pd.DataFrame, 'X_train'], Annotated[pd.DataFrame, 'X_test'], Annotated[pd.Series, 'y_train'], Annotated[pd.Series, 'y_test']]:\n",
    "    \"\"\"\n",
    "    Split the data into train and test sets.\n",
    "\n",
    "    Args:\n",
    "        features (pd.DataFrame): The input data.\n",
    "        target (pd.Series) : Target colun\n",
    "        test_size (float): The proportion of the data to include in the test set. Default is 0.2.\n",
    "        random_state (int): The seed for the random number generator. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: The train and test sets.\n",
    "    \"\"\"\n",
    "    logging.info(\"Splitting data...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=test_size, random_state=random_state)\n",
    "    logging.info(\"Data split successfully.\")\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c28fac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ingest_data(data_source='/home/skhapijulhossen/Desktop/demand-forecast-SQGroup/data/sales_bya.parquet')\n",
    "df = clean_data(data=df)\n",
    "targets, static_features, application_group, uses, mkt = encode_and_aggregate_data(data=df)\n",
    "features, target, imputer = merge_all_features(targets, static_features, application_group, uses, mkt)\n",
    "X_train, X_test, y_train, y_test = split_data(features=features, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a78fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b048d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlet_id</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>1.237182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>1.195289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>1.352732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>1.359670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>1.179050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3746</td>\n",
       "      <td>1.302701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4151</td>\n",
       "      <td>1.216001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4385</td>\n",
       "      <td>1.127496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>7528</td>\n",
       "      <td>1.081250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>7529</td>\n",
       "      <td>1.527778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    outlet_id     grade\n",
       "0        1001  1.237182\n",
       "1        1002  1.195289\n",
       "2        1003  1.352732\n",
       "3        1004  1.359670\n",
       "4        1005  1.179050\n",
       "..        ...       ...\n",
       "56       3746  1.302701\n",
       "57       4151  1.216001\n",
       "58       4385  1.127496\n",
       "59       7528  1.081250\n",
       "60       7529  1.527778\n",
       "\n",
       "[61 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_features.groupby(by='outlet_id').mean()[['grade']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aa8599b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "1    0.821150\n",
       "2    0.152982\n",
       "3    0.022640\n",
       "4    0.003227\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.grade.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249640ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f258b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_group.to_parquet('application_group.parquet', index=False)\n",
    "uses.to_parquet('uses.parquet', index=False)\n",
    "mkt.to_parquet('mkt.parquet', index=False)\n",
    "static_features.groupby(by='outlet_id').mean()[['ecoind', 'noc']].reset_index().to_parquet('ecoind_noc.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e4f2b404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27681, 64), (27681,))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d94b0d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9227, 64), (9227,))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bf8b02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f3779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f4f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed353a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34579ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Rich', 'Industry', 'House Wiring', 'Fan & Lighting Connection',\n",
       "       'Air Condition & Washing Machine, Heavy Item', 'Earthing',\n",
       "       'Industry, Machineries', 'Urban', 'Rural', 'Semi Urban', 'Others',\n",
       "       'wire', 'fy', 'grade', 'noc', 'dfc', 'area_km2', 'literacy_rate_perc',\n",
       "       'pcx', 'excnts', 'exach', 'trc', 'tlcolt', 'tmtm', 'ecoind', 'sf',\n",
       "       'sop', 'pminx', 'tms_cr', 'mas', 'kpi', 'timestamp_quarter',\n",
       "       'timestamp_day_of_week', 'timestamp_day_of_month',\n",
       "       'timestamp_month_start', 'timestamp_month_end',\n",
       "       'timestamp_quarter_start', 'timestamp_quarter_end',\n",
       "       'timestamp_year_start', 'timestamp_year_end', 'net_price_lag_3',\n",
       "       'qtym_lag_8', 'net_price_lag_16', 'qtym_lag_24', 'qtym_window_24_mean',\n",
       "       'qtym_expanding_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = SmartCorrelatedSelection(variables=None, method='pearson', threshold=0.8, missing_values='ignore', selection_method='missing_values', estimator=None, scoring='r2', cv=5, confirm_variables=False)\n",
    "best = sm.fit_transform(X_train, y_train)\n",
    "best.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3fa3540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5900\n",
      "[LightGBM] [Info] Number of data points in the train set: 27681, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 526.995830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 63,  46,  35, 117,  21,  27,  32,   0,  15,  49,  24,  34,  31,\n",
       "        28,  45,  21,  69,  17,  12,  11,  29,  38,  20,  36,  52,  49,\n",
       "        69,  36,  24, 726,   1,   6,  12,  10, 220, 287,  23,  11,   0,\n",
       "         0,  16,   7,  25,  13,   0,   0,   0,   0,   0,   0,   0,  83,\n",
       "        53,  34,  27,  92,  59,  56,  24,  50,  33,  43,  39], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = LGBMRegressor()\n",
    "selector.fit(X_train.values, y_train)\n",
    "selector.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e47206d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 40, 41, 42, 43, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n",
       "       61, 62])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = selector.feature_importances_ > 0\n",
    "np.where(selected)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "928b2263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['outlet_id', 'General', 'Moderate', 'Rich', 'Industry', 'House Wiring',\n",
       "       'Fan & Lighting Connection', 'Lift & Heavy Item', 'Earthing',\n",
       "       'Industry, Machineries', 'Urban', 'Rural', 'Semi Urban', 'Others',\n",
       "       'wire', 'rm', 'fy', 'grade', 'noc', 'dfc', 'area_km2', 'population',\n",
       "       'literacy_rate_perc', 'pcx', 'excnts', 'exach', 'trc', 'tlcolt', 'tmtm',\n",
       "       'ecoind', 'sf', 'sop', 'pminx', 'tms_cr', 'mas', 'kpi',\n",
       "       'timestamp_month', 'timestamp_week', 'timestamp_day_of_week',\n",
       "       'timestamp_day_of_month', 'timestamp_day_of_year', 'net_price_lag_3',\n",
       "       'qtym_lag_3', 'net_price_lag_8', 'qtym_lag_8', 'net_price_lag_16',\n",
       "       'qtym_lag_16', 'net_price_lag_24', 'qtym_lag_24',\n",
       "       'net_price_window_24_mean', 'qtym_window_24_mean',\n",
       "       'net_price_expanding_std', 'qtym_expanding_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_features = X_train.columns[np.where(selected)[0]]\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a952241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36908, 64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_features = list(set(best.columns).intersection(set(best_features)))\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7513e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "\n",
    "def decompose_features_kernel_pca(X, n_components=2, kernel='linear', load=False):\n",
    "    \"\"\"\n",
    "    Decomposes features using PCA.\n",
    "\n",
    "    Args:\n",
    "    - X (array-like): Input feature matrix.\n",
    "    - n_components (int): Number of components (default: 2).\n",
    "\n",
    "    Returns:\n",
    "    - X_transformed (array-like): Transformed feature matrix.\n",
    "    \"\"\"\n",
    "    if load:\n",
    "        pass\n",
    "    \n",
    "    # Initialize KernelPCA object\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    # Fit and transform the input feature matrix\n",
    "    X_transformed = pca.fit_transform(X)\n",
    "\n",
    "    return X_transformed, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e2f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12957b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260cead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ee507b1",
   "metadata": {},
   "source": [
    "<center><b> Experiments </b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52512e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    ") -> Annotated[float, 'r2_score']:\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_pred = model.predict(X).reshape(y.shape[0], 1)\n",
    "\n",
    "        # MAPE, MSE, RMSE, R2, AIC, BIC\n",
    "        mape = mean_absolute_percentage_error(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        mlflow.log_metrics(dict(mape=mape, mse=mse, r2=r2))\n",
    "        return r2\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33631bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.create_experiment('Net Price Forecasting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62bf1733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3419\n",
      "[LightGBM] [Info] Number of data points in the train set: 22144, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 526.203339\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3415\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 532.217625\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 528.202506\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3409\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 524.829758\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3451\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 523.525887\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3419\n",
      "[LightGBM] [Info] Number of data points in the train set: 22144, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 526.203339\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3415\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 532.217625\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 528.202506\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3409\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 524.829758\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3451\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 523.525887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3419\n",
      "[LightGBM] [Info] Number of data points in the train set: 22144, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 526.203339\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3415\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 532.217625\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 528.202506\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3409\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 524.829758\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3451\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 523.525887\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3419\n",
      "[LightGBM] [Info] Number of data points in the train set: 22144, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 526.203339\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001874 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3415\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 532.217625\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 528.202506\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001908 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3409\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 524.829758\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3451\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 523.525887\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001915 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3419\n",
      "[LightGBM] [Info] Number of data points in the train set: 22144, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 526.203339\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3415\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 532.217625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 528.202506\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3409\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 524.829758\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001937 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3451\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 523.525887\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3419\n",
      "[LightGBM] [Info] Number of data points in the train set: 22144, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 526.203339\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000574 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3415\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 532.217625\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 528.202506\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002078 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3409\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 524.829758\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3451\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 523.525887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3419\n",
      "[LightGBM] [Info] Number of data points in the train set: 22144, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 526.203339\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3415\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 532.217625\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 528.202506\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3409\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 524.829758\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3451\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 523.525887\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3419\n",
      "[LightGBM] [Info] Number of data points in the train set: 22144, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 526.203339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3415\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 532.217625\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 528.202506\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3409\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 524.829758\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3451\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 523.525887\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3419\n",
      "[LightGBM] [Info] Number of data points in the train set: 22144, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 526.203339\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3415\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 532.217625\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 528.202506\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3409\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 524.829758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3451\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 523.525887\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3419\n",
      "[LightGBM] [Info] Number of data points in the train set: 22144, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 526.203339\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3415\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 532.217625\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3418\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 528.202506\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001899 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3409\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 524.829758\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3451\n",
      "[LightGBM] [Info] Number of data points in the train set: 22145, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 523.525887\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3427\n",
      "[LightGBM] [Info] Number of data points in the train set: 27681, number of used features: 38\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 526.995830\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment('Net Price Forecasting')\n",
    "with mlflow.start_run() as pr:\n",
    "    XGB_PARAMS_SPACE = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.1, 0.3, 0.5],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'subsample': [0.5, 0.7, 0.9],\n",
    "        'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "        'gamma': [0, 0.1, 0.2, 0.01]\n",
    "    }\n",
    "#     with mlflow.start_run(run_name=\"XGBoost Run\", nested=True) as xgb_run:\n",
    "#         # Randomized Grid Search for XGBoost hyperparameters\n",
    "#         grid = RandomizedSearchCV(\n",
    "#             XGBRFRegressor(),\n",
    "#             param_distributions=XGB_PARAMS_SPACE,\n",
    "#             scoring='r2',\n",
    "#             cv=5,\n",
    "#             verbose=0\n",
    "#         )\n",
    "#         grid.fit(X_train[best_features].values, y_train)\n",
    "#         model = grid.best_estimator_\n",
    "#         mlflow.log_params(grid.best_params_)\n",
    "#         mlflow.log_param('components', len(best_features))\n",
    "#         mlflow.log_param('columns', best_features)\n",
    "#         ### log metrics\n",
    "#         r2 = evaluate_model(grid.best_estimator_, X_test[best_features].values, y_test)\n",
    "#         if r2 > 0.7:\n",
    "#             mlflow.xgboost.log_model(model, f'XGB-QTYM')\n",
    "    \n",
    "    LGB_PARAMS_SPACE = {\n",
    "        'boosting_type': ['gbdt', 'dart', 'goss'],\n",
    "        'num_leaves': [20, 30, 40, 50, 60],\n",
    "        'max_depth': [5, 7, 9, 11, 15, 21],  # -1 means no limit\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0.0, 0.1, 0.5, 1.0, 0.01],\n",
    "        'reg_lambda': [0.0, 0.1, 0.5, 1.0],\n",
    "        'min_child_samples': [20, 30, 40, 50, 60],\n",
    "        'random_state': [42]  # Ensure reproducibility\n",
    "    }\n",
    "    with mlflow.start_run(run_name=\"LGB Run\", nested=True) as lgb_run:\n",
    "        # Randomized Grid Search for XGBoost hyperparameters\n",
    "        grid = RandomizedSearchCV(\n",
    "            LGBMRegressor(),\n",
    "            param_distributions=LGB_PARAMS_SPACE,\n",
    "            scoring='r2',\n",
    "            cv=5,\n",
    "            verbose=0\n",
    "        )\n",
    "        grid.fit(X_train[best_features].values, y_train)\n",
    "        model = grid.best_estimator_\n",
    "        mlflow.log_params(grid.best_params_)\n",
    "        mlflow.log_param('components', len(best_features))\n",
    "        mlflow.log_param('columns', best_features)\n",
    "\n",
    "        ### log metrics\n",
    "        r2 = evaluate_model(grid.best_estimator_, X_test[best_features].values, y_test)\n",
    "        if r2 > 0.7:\n",
    "            mlflow.lightgbm.log_model(model, f'LGB-QTYM-{r2}')\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dabd3fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0d195ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlflow.pyfunc.loaded_model:\n",
       "  artifact_path: LGB-QTYM-0.7456832147065215\n",
       "  flavor: mlflow.lightgbm\n",
       "  run_id: ae501105c42f4987b526ea740568507a"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9ab2f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresN = ['net_price_lag_16', 'Earthing', 'Urban', 'Others', 'grade', 'pminx', 'excnts', 'Fan & Lighting Connection', 'kpi', 'sop', 'Rural', 'exach', 'qtym_window_24_mean', 'Rich', 'tmtm', 'Semi Urban', 'tlcolt', 'timestamp_day_of_month', 'sf', 'mas', 'noc', 'timestamp_day_of_week', 'fy', 'ecoind', 'tms_cr', 'literacy_rate_perc', 'area_km2', 'qtym_expanding_std', 'qtym_lag_24', 'pcx', 'Industry', 'dfc', 'House Wiring', 'wire', 'net_price_lag_3', 'Industry, Machineries', 'qtym_lag_8', 'trc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b20e4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[featuresN+['target', 'outlet_id', 'timestamp']].to_parquet('processed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "412acf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(829, 38)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('processed.parquet')\n",
    "df.loc[df.outlet_id==1001, featuresN].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6b03481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_by_outlet(outlet_id, DATA):\n",
    "    # Assuming your model has a function to load data based on outlet_id\n",
    "    filtered = DATA.loc[DATA.outlet_id==outlet_id]\n",
    "    return filtered[best_features], filtered['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0fa7810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_data_by_outlet(1001, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cfca2e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1589.31018984,  200.3760891 ,  879.93367635, ...,   98.30341263,\n",
       "        577.34312924,  143.15133717])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(df[best_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "596d021d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1596.651245\n",
       "1         199.931656\n",
       "2         876.108337\n",
       "3        2332.785156\n",
       "4         977.241394\n",
       "            ...     \n",
       "36903     646.000000\n",
       "36904     157.666672\n",
       "36905      94.000000\n",
       "36906     593.571411\n",
       "36907     150.000000\n",
       "Name: target, Length: 36908, dtype: float32"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c412b8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlflow.pyfunc.PyFuncModel"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1db7cc55",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PyFuncModel' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PyFuncModel' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "loaded_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "450a5aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1919\n",
      "[LightGBM] [Info] Number of data points in the train set: 829, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 755.026792\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 124,    0,    0,    0,   66,    0,    0,    0,    0,    0,    0,\n",
       "          0,  142,    0, 1482,    0,    0,  146,    0,    0,    0,   50,\n",
       "         28,    0,    0,    0,    0,  241,  105,    8,    0,    0,    0,\n",
       "         85,  385,    0,  127,    0], dtype=int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBMRegressor().fit(x.values, y).feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eedb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = selector.feature_importances_ > 0\n",
    "np.where(selected)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
