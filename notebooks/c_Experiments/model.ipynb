{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5811e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e08fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder, RobustScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from feature_engine.timeseries.forecasting import LagFeatures, WindowFeatures, ExpandingWindowFeatures\n",
    "from feature_engine.datetime import DatetimeFeatures\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54865cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Annotated, Tuple, Dict, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d969eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde7f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774511ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fe7a110",
   "metadata": {},
   "source": [
    "<center><b>Process Data</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2265884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(data_source: Annotated[str, 'data_source']) -> Annotated[pd.DataFrame, 'data']:\n",
    "    \"\"\"\n",
    "    Ingests data from a given path.\n",
    "\n",
    "    Args:\n",
    "        data_source: The path to the data.\n",
    "\n",
    "    Returns:\n",
    "        The data as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Reading data from {data_source}\")\n",
    "        data = pd.read_parquet(data_source)\n",
    "        logging.info(f\"Data read from {data_source}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading data from {data_source}: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce909ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data: Annotated[pd.DataFrame, 'data']) -> Annotated[pd.DataFrame, 'cleaned_data']:\n",
    "    \"\"\"\n",
    "    Clean the data by removing duplicates, null values, and converting columns to appropriate types.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned data. None if an error occurs.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Cleaning data...\")\n",
    "        data.drop_duplicates(keep='last', inplace=True)\n",
    "        data.dropna(inplace=True)\n",
    "        data.drop(columns=['wire.1','client_id', 'CID', 'Base Size'], inplace=True)\n",
    "        \n",
    "        # format the date time\n",
    "        data['date'] = pd.to_datetime(data.date).values\n",
    "\n",
    "        # Sort\n",
    "        data.sort_values(by='date', inplace=True)\n",
    "\n",
    "        # renaming cols\n",
    "        data.columns = [col.lower().strip().replace(' ', '_')\n",
    "                        for col in data.columns]\n",
    "        data.rename(\n",
    "            {'area_(km)^2': 'area_km2', 'population_(approx.)': 'population',\n",
    "             'literacy_rate_(%)': 'literacy_rate_perc'},\n",
    "            axis=1, inplace=True)\n",
    "\n",
    "        # optimizing for memory\n",
    "        for col in data.select_dtypes('float64').columns:\n",
    "            data[col] = data[col].astype('float32')\n",
    "\n",
    "        for col in data.select_dtypes('int64').columns:\n",
    "            data[col] = data[col].astype('int32')\n",
    "\n",
    "        # literacy rate conversion\n",
    "        data['literacy_rate_perc'] = data.literacy_rate_perc.astype('float32')\n",
    "\n",
    "        # rename date -> timestamp\n",
    "        data.rename({'date': 'timestamp'}, axis=1, inplace=True)\n",
    "\n",
    "        logging.info(\"Data cleaned.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error cleaning data: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "978bb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_aggregate_data(\n",
    "    data: Annotated[pd.DataFrame, 'cleaned_data']\n",
    ") -> Tuple[Annotated[pd.DataFrame, 'target'], Annotated[pd.DataFrame, 'static features'], Annotated[pd.DataFrame, 'aggregated application_group table'], Annotated[pd.DataFrame, 'aggregated uses tabe'], Annotated[pd.DataFrame, 'aggregated mkt table']]:\n",
    "    \"\"\"\n",
    "    Encode categorical features of the data.\n",
    "\n",
    "    Args:\n",
    "        data: Dataframe containing the categorical features.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe containing the encoded categorical features OR None.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info('Encoding categorical features...')\n",
    "        \n",
    "        # HASH FEATURES category\n",
    "        data['category'] = data.category.apply(\n",
    "            lambda cat: {'Domestic': 1, 'Power': 0}[cat])\n",
    "        data['grade'] = data.grade.apply(\n",
    "            lambda cat: {'Grade1': 1, 'Grade2': 2,\n",
    "                         'Grade3': 3, 'Grade4': 4}[cat]\n",
    "        )\n",
    "        data['ecoind'] = data.ecoind.apply(\n",
    "            lambda cat: {'Medium': 2, 'High': 4, 'Low': 2, 'Poor': 1}[cat]\n",
    "        )\n",
    "\n",
    "        # OneHot Encoding\n",
    "        data = pd.get_dummies(data, columns=['division', 'region'])\n",
    "        \n",
    "        # renaming cols\n",
    "        data.columns = [col.lower().strip().replace(' ', '_')\n",
    "                        for col in data.columns]\n",
    "        \n",
    "        for column in data.select_dtypes('bool').columns:\n",
    "            data[column] = data[column].astype(int)\n",
    "    \n",
    "\n",
    "        # optimize for memory\n",
    "        for col in data.select_dtypes('int64').columns:\n",
    "            data[col] = data[col].astype('int32')\n",
    "\n",
    "        # Aggregate targets by outlet_id\n",
    "        targets = df.pivot_table(index=['timestamp','outlet_id'], aggfunc={\n",
    "                'net_price': 'mean',\n",
    "                'qtym': 'mean',\n",
    "            }\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Aggreate static feature by outlet_id\n",
    "        static_features = df[['timestamp','outlet_id','wire', 'rm',\n",
    "           'fy', 'grade', 'noc',\n",
    "           'dfc', 'area_km2', 'population', 'literacy_rate_perc', 'pcx', 'excnts',\n",
    "           'exach', 'trc', 'tlcolt', 'tmtm', 'ecoind', 'sf', 'sop', 'pminx',\n",
    "           'tms_cr', 'mas', 'kpi', ]].groupby(by=['timestamp','outlet_id'],).mean().reset_index()\n",
    "        \n",
    "        # aggreated appliatin group by outlet_id\n",
    "        application_group = pd.DataFrame(columns=['General', 'Moderate', 'Rich', 'Industry'])\n",
    "        for outlet in df.outlet_id.value_counts().index:\n",
    "            ratio = df.loc[df.outlet_id==outlet, 'application_group'].value_counts(normalize=True).to_dict()\n",
    "            application_group.loc[outlet] = ratio\n",
    "        application_group = application_group.fillna(0).reset_index().rename(columns={'index':'outlet_id'}).astype(np.float32)\n",
    "        \n",
    "        # Aggregated uses by outlet_id\n",
    "        uses = pd.DataFrame(columns=[\n",
    "            'House Wiring', 'Fan & Lighting Connection',\n",
    "            'Air Condition & Washing Machine, Heavy Item', 'Lift & Heavy Item',\n",
    "            'Earthing', 'Industry, Machineries'\n",
    "            ]\n",
    "        )\n",
    "        for outlet in df.outlet_id.value_counts().index:\n",
    "            ratio = df.loc[df.outlet_id==outlet, 'uses'].value_counts(normalize=True).to_dict()\n",
    "            uses.loc[outlet] = ratio\n",
    "        uses = uses.fillna(0).reset_index().rename(columns={'index':'outlet_id'}).astype(np.float32)\n",
    "        \n",
    "        # Aggregated mkt ratio by outlet_id\n",
    "        mkt = pd.DataFrame(columns=\n",
    "            ['Urban', 'Rural', 'Semi Urban', 'Others']\n",
    "        )\n",
    "        for outlet in df.outlet_id.value_counts().index:\n",
    "            ratio = df.loc[df.outlet_id==outlet, 'mkt'].value_counts(normalize=True).to_dict()\n",
    "            mkt.loc[outlet] = ratio\n",
    "        mkt = mkt.fillna(0).reset_index().rename(columns={'index':'outlet_id'}).astype(np.float32)\n",
    "        logging.info('Encoding categorical features completed.')\n",
    "        return targets, static_features, application_group, uses, mkt\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error encoding categorical features: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d32640a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddTemporalFeatures(targets: Annotated[pd.DataFrame, 'encoded data']) -> Annotated[pd.DataFrame, 'temporal features']:\n",
    "    features_to_extract = [\n",
    "        \"month\", \"quarter\", \"semester\", \"week\", \"day_of_week\", \"day_of_month\",\n",
    "        \"day_of_year\", \"weekend\", \"month_start\", \"month_end\", \"quarter_start\",\n",
    "        \"quarter_end\", \"year_start\", \"year_end\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        logging.info(f'==> Processing AddTemporalFeatures()')\n",
    "        temporal = DatetimeFeatures(\n",
    "            features_to_extract=features_to_extract).fit_transform(targets[['timestamp']])\n",
    "        # for col in temporal.columns:\n",
    "        #     data.loc[:, col] = temporal[col].values\n",
    "        logging.info(f'==> Successfully processed AddTemporalFeatures()')\n",
    "        return temporal\n",
    "    except Exception as e:\n",
    "        logging.error(f'==> Error in AddTemporalFeatures()')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698c7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddLagFeatures(targets: Annotated[pd.DataFrame, 'after added temporal features']) -> Annotated[pd.DataFrame, 'Lag features']:\n",
    "    \"\"\"\n",
    "    Add lag features to the data.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Adding lag features to the data.\")\n",
    "    try:\n",
    "        # Add Lag  Feature\n",
    "        lagfeatures = LagFeatures(variables=None, periods=[3, 8, 16, 24], freq=None, sort_index=True,\n",
    "                                  missing_values='raise', drop_original=False)\n",
    "        lagfeatures.fit(targets[['timestamp', 'net_price', 'qtym']])\n",
    "        features = lagfeatures.transform(\n",
    "            targets[['timestamp', 'net_price', 'qtym']])\n",
    "        # for col in list(features.columns)[3:]:\n",
    "        #     data[col] = features[col].values\n",
    "        logging.info(f'==> Successfully processed add_lag_features()')\n",
    "        return features.drop(['timestamp', 'net_price', 'qtym'], axis=1)\n",
    "    except Exception as e:\n",
    "        logger.error(f'in The add_lag_features(): {e}')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e2246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddWindowFeatures(targets: Annotated[pd.DataFrame, 'After lag features added']) -> Annotated[pd.DataFrame, 'window features']:\n",
    "    \"\"\"Add window features to the dataframe\n",
    "\n",
    "    Args:\n",
    "        data (Union[dd.DataFrame, pd.DataFrame]): The dataframe to add window features to.\n",
    "\n",
    "    Returns:\n",
    "        Union[dd.DataFrame, pd.DataFrame]: The dataframe with window features added.\n",
    "    \"\"\"\n",
    "    logging.info(\"Adding window features to the dataframe\")\n",
    "\n",
    "    try:\n",
    "        windowfeatures = WindowFeatures(variables=None, window=24, freq=None, sort_index=True,\n",
    "                                        missing_values='raise', drop_original=False)\n",
    "        windowfeatures.fit(\n",
    "            targets[['timestamp', 'net_price', 'qtym']])\n",
    "        features = windowfeatures.transform(\n",
    "            targets[['timestamp', 'net_price', 'qtym']])\n",
    "        # for col in list(features.columns)[3:]:\n",
    "        #     data[col] = features[col].values\n",
    "        logging.info(f'==> Successfully processed add_window_features()')\n",
    "        return features.drop(['timestamp', 'net_price', 'qtym'], axis=1)\n",
    "    except Exception as e:\n",
    "        logging.error(f'in add_window_features(): {e}')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c0f77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddExpWindowFeatures(targets: Annotated[pd.DataFrame, 'after added temporal features']) -> Annotated[pd.DataFrame, 'added Expanding Window features']:\n",
    "    \"\"\"Add Expanding Window Features to the data.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data.\n",
    "    Returns:\n",
    "        pd.DataFrame: The data with added expanding window features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        expwindow = ExpandingWindowFeatures(\n",
    "            variables=None, min_periods=7, functions='std', \n",
    "            periods=7, freq=None, sort_index=True, \n",
    "            missing_values='raise', drop_original=False\n",
    "        )\n",
    "        features = expwindow.fit_transform(targets[['timestamp', 'net_price', 'qtym']])\n",
    "        \n",
    "        # # \n",
    "        # for col in list(features.columns)[3:]:\n",
    "        #     data[col] = features[col].values\n",
    "        return features.drop(['timestamp', 'net_price', 'qtym'], axis=1)\n",
    "    except Exception as e:\n",
    "        logging.error(f'in The add_expw_features(): {e}')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "312f9bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeAllFeatures(\n",
    "    targets: Annotated[pd.DataFrame, 'targets'], \n",
    "    static_features: Annotated[pd.DataFrame, 'static_features'],\n",
    "    application_group: Annotated[pd.DataFrame, 'application_group'],\n",
    "    uses: Annotated[pd.DataFrame, 'uses'],\n",
    "    mkt: Annotated[pd.DataFrame, 'mkt'],\n",
    "    \n",
    ") -> Tuple[Annotated[pd.DataFrame, 'features'], Annotated[pd.Series, 'target']]:\n",
    "    \"\"\"Merges All Features into One.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data.\n",
    "    Returns:\n",
    "        pd.DataFrame: The data with added expanding window features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f'==> merging features...')\n",
    "        \n",
    "        # Generate outlet wise timeseries_features\n",
    "        timeseries_features_outlet_wise = pd.DataFrame()\n",
    "        for outlet_id in targets.outlet_id.value_counts().index:\n",
    "            outlet_wise = targets.loc[targets.outlet_id==outlet_id]\n",
    "            temporal = AddTemporalFeatures(outlet_wise)\n",
    "            lag_features = AddLagFeatures(outlet_wise)\n",
    "            window_features = AddWindowFeatures(outlet_wise)\n",
    "            exp_window_features = AddExpWindowFeatures(outlet_wise)\n",
    "            outlet_wise_features = pd.concat([outlet_wise[['timestamp','outlet_id',]], temporal, lag_features, window_features, exp_window_features], axis=1)\n",
    "            timeseries_features_outlet_wise = pd.concat([timeseries_features_outlet_wise, outlet_wise_features], ignore_index=True)\n",
    "        \n",
    "        # Merge outlet wise timeseries_features\n",
    "        targets.merge(timeseries_features_outlet_wise, on=['timestamp','outlet_id',], how='inner')\n",
    "\n",
    "        \n",
    "        # Merge application group, uses, mkt\n",
    "        data = targets.merge(\n",
    "            application_group, on='outlet_id', how='inner'\n",
    "        ).merge(\n",
    "            uses, on='outlet_id', how='inner'\n",
    "        ).merge(\n",
    "            mkt, on='outlet_id', how='inner'\n",
    "        ).merge(\n",
    "            static_features, on=['timestamp', 'outlet_id'], how='inner'\n",
    "        ).merge(\n",
    "            timeseries_features_outlet_wise, on=['timestamp', 'outlet_id'], how='inner')\n",
    "        \n",
    "        # Drops Nulls\n",
    "        data.bfill(inplace=True)\n",
    "        return data.drop(columns=['timestamp', 'net_price', 'qtym', 'outlet_id']), data['qtym']\n",
    "    except Exception as e:\n",
    "        logging.error(f'==> Error when merging features: {e}')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e93e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(features: Annotated[pd.DataFrame, 'features to scale'],) -> Annotated[pd.DataFrame, 'standardized features']:\n",
    "    \"\"\"Scaling step.\n",
    "    Args:\n",
    "        data: Input data.\n",
    "    Returns:\n",
    "        Normalized data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f'==> Processing scale_data()')\n",
    "        # Assuming the data is a pandas DataFrame\n",
    "        scaler = RobustScaler(\n",
    "            with_centering=True,\n",
    "            with_scaling=True,\n",
    "            quantile_range=(25.0, 75.0),\n",
    "            copy=True,\n",
    "            unit_variance=False,\n",
    "        )\n",
    "        scaler.fit(features)\n",
    "        features = pd.DataFrame(scaler.transform(features), columns=features.columns)\n",
    "        scaler.fit(features)\n",
    "        \n",
    "#         # save Scaler model\n",
    "#         joblib.dump(scaler, os.path.join(config.ARTIFACTS_DIR, 'scaler.pkl'))\n",
    "#         logger.info(\n",
    "#             f'Scaler model saved to {os.path.join(config.ARTIFACTS_DIR, \"scaler.pkl\")}')\n",
    "#         features.to_parquet(config.FEATURE_STORE, index=False)\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        logging.error(f\"in scale_data(): {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c4d9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    features: Annotated[pd.DataFrame, 'features'],\n",
    "    target: Annotated[pd.Series, 'target'],\n",
    "    test_size: float = 0.25,\n",
    "    random_state: int = 33\n",
    ") -> Tuple[Annotated[pd.DataFrame, 'X_train'], Annotated[pd.DataFrame, 'X_test'], Annotated[pd.Series, 'y_train'], Annotated[pd.Series, 'y_test']]:\n",
    "    \"\"\"\n",
    "    Split the data into train and test sets.\n",
    "\n",
    "    Args:\n",
    "        features (pd.DataFrame): The input data.\n",
    "        target (pd.Series) : Target colun\n",
    "        test_size (float): The proportion of the data to include in the test set. Default is 0.2.\n",
    "        random_state (int): The seed for the random number generator. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: The train and test sets.\n",
    "    \"\"\"\n",
    "    logging.info(\"Splitting data...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=test_size, random_state=random_state)\n",
    "    logging.info(\"Data split successfully.\")\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c28fac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ingest_data(data_source='/demand-forecast-SQGroup/data/data_bya_series.parquet')\n",
    "df = clean_data(data=df)\n",
    "targets, static_features, application_group, uses, mkt = encode_and_aggregate_data(data=df)\n",
    "features, target = mergeAllFeatures(targets, static_features, application_group, uses, mkt)\n",
    "std_features = scale_data(features=features)\n",
    "X_train, X_test, y_train, y_test = split_data(features=std_features, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4f2b404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27996, 62), (27996,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d94b0d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9333, 62), (9333,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf8b02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfe = RFECV(\n",
    "#     SVR(kernel='linear'),\n",
    "#     step=1,\n",
    "#     min_features_to_select=int(np.sqrt(features.shape[1])),\n",
    "#     cv=3,\n",
    "#     scoring='r2',\n",
    "#     verbose=1,\n",
    "#     n_jobs=None,\n",
    "#     importance_getter='coef_',\n",
    "# )\n",
    "# rfe.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34579ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SmartCorrelatedSelection(variables=None, method='pearson', threshold=0.8, missing_values='ignore', selection_method='missing_values', estimator=None, scoring='r2', cv=5, confirm_variables=False)\n",
    "best = sm.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7513e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "\n",
    "def decompose_features_kernel_pca(X, n_components=2, kernel='linear', load=False):\n",
    "    \"\"\"\n",
    "    Decomposes features using KernelPCA.\n",
    "\n",
    "    Args:\n",
    "    - X (array-like): Input feature matrix.\n",
    "    - n_components (int): Number of components (default: 2).\n",
    "    - kernel (str): Kernel function to use ('linear', 'poly', 'rbf', 'sigmoid', or a callable) (default: 'linear').\n",
    "\n",
    "    Returns:\n",
    "    - X_transformed (array-like): Transformed feature matrix.\n",
    "    \"\"\"\n",
    "    if load:\n",
    "        pass\n",
    "    \n",
    "    # Initialize KernelPCA object\n",
    "    kpca = PCA(n_components=20)\n",
    "    \n",
    "    # Fit and transform the input feature matrix\n",
    "    X_transformed = kpca.fit_transform(X)\n",
    "\n",
    "    return X_transformed, kpca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d8e2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_f, pca = decompose_features_kernel_pca(best, n_components=int(np.sqrt(best.shape[1])), kernel='linear', load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c12957b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.7522293 ,  0.98296154,  0.51003387, ..., -0.14732248,\n",
       "        -0.40549644, -0.1982893 ],\n",
       "       [ 1.70618202, -1.18945608,  1.29284123, ..., -0.32820525,\n",
       "        -0.1185912 ,  0.48002742],\n",
       "       [ 0.0939029 ,  1.70894291, -0.08137184, ...,  1.34855327,\n",
       "        -1.43384542, -0.74639628],\n",
       "       ...,\n",
       "       [ 1.02862341, -3.84349677,  1.06624677, ..., -0.95197557,\n",
       "        -0.36398289,  0.06009521],\n",
       "       [ 1.08015896, -1.13031837, -1.57221804, ..., -0.6510743 ,\n",
       "         0.06035134, -0.20690283],\n",
       "       [-2.37712072,  1.32586295,  1.3178424 , ...,  0.44071855,\n",
       "        -0.74486799, -0.43703693]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcs_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260cead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ee507b1",
   "metadata": {},
   "source": [
    "<center><b> Experiments </b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52512e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_pred = model.predict(X).reshape(y.shape[0], 1)\n",
    "\n",
    "        # MAPE, MSE, RMSE, R2, AIC, BIC\n",
    "        mape = mean_absolute_percentage_error(y, y_pred)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        mlflow.log_metrics(dict(mape=mape, mse=mse, r2=r2))\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33631bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.create_experiment('Net Price Forecasting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62bf1733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\demand-forecast-SQGroup\\zenenv\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:43:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "D:\\demand-forecast-SQGroup\\zenenv\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "D:\\demand-forecast-SQGroup\\zenenv\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment('Net Price Forecasting')\n",
    "with mlflow.start_run():\n",
    "    XGB_PARAMS_SPACE = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.1, 0.3, 0.5],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'subsample': [0.5, 0.7, 0.9],\n",
    "        'colsample_bytree': [0.5, 0.7, 0.9],\n",
    "        'gamma': [0, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "       \n",
    "    # Randomized Grid Search for XGBoost hyperparameters\n",
    "    grid = RandomizedSearchCV(\n",
    "        XGBRFRegressor(),\n",
    "        param_distributions=XGB_PARAMS_SPACE,\n",
    "        scoring='r2',\n",
    "        cv=5,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid.fit(pcs_f, y_train)\n",
    "    model = grid.best_estimator_\n",
    "    mlflow.log_params(grid.best_params_)\n",
    "    mlflow.xgboost.log_model(model, 'XGBRF-QTYM')\n",
    "    ### log metrics\n",
    "    evaluate_model(grid.best_estimator_, pca.transform(X_test[best.columns]), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd3fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d195ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.782329983125268"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca2e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d021d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412b8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
